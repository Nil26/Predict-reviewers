{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"leading_reviewer_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "reviewer_path_list = glob(data_path+\"*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use 10 reviews for each reviewer (pick out the 10 largest size of reviews)\n",
    "reviewers = []\n",
    "reviewers_review = []\n",
    "reviewers_literature = []\n",
    "for reviewer_path in reviewer_path_list:\n",
    "    reviewers.append(re.search(r'leading_reviewer_data\\\\([a-zA-Z_-]*)\\\\',reviewer_path).group(1))\n",
    "    current_reviewer_review = [f for f in os.listdir(reviewer_path) if '.txt' in f]\n",
    "    # select the 10 largest size of reviews\n",
    "    current_selected_review_index = np.argsort([os.stat(reviewer_path+f).st_size for f in os.listdir(reviewer_path) if '.txt' in f])[::-1][:10]\n",
    "    current_reviewer_review_selected = list(np.array(current_reviewer_review)[ind])\n",
    "    # get the review texts\n",
    "    review_texts = []\n",
    "    for nm in current_reviewer_review_selected:\n",
    "        with open(reviewer_path+nm, encoding=\"utf8\") as f:\n",
    "            text_to_append = f.read().replace('\\n',' ')\n",
    "            review_texts.append(text_to_append)\n",
    "    reviewers_review.append(review_texts)\n",
    "    \n",
    "    current_reviewer_literature = [f for f in os.listdir(reviewer_path+\"training_data\\\\\") if '.txt' in f]\n",
    "    # get the literature texts\n",
    "    literature_texts = []\n",
    "    for nm in current_reviewer_literature:\n",
    "        with open(reviewer_path+\"training_data\\\\\"+nm, encoding=\"utf8\") as f:\n",
    "            text_to_append = f.read().replace('\\n',' ')\n",
    "            literature_texts.append(text_to_append)\n",
    "    reviewers_literature.append(literature_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build training set, development set and test set\n",
    "## Randomly picking the test data from the whole dataset\n",
    "total: 25 reviewers\n",
    "\n",
    "each reviewer has 10 reviews and 10 literatures.\n",
    "\n",
    "Test set: half of the reviews (each reviewer has 5 reviews)\n",
    "\n",
    "Development set: the other half of the reviews\n",
    "\n",
    "Training set: all the literatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "dev_set = []\n",
    "test_set = []\n",
    "for reviewer in reviewers_review:\n",
    "    # copy() is important here, or the reviewers_review will be shuffled\n",
    "    reviewer_cp = reviewer.copy()\n",
    "    np.random.shuffle(reviewer_cp)\n",
    "    dev_set.append(reviewer_cp[:5])\n",
    "    test_set.append(reviewer_cp[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical and punctuation features\n",
    "\n",
    "Lexical features:\n",
    "\n",
    "The average number of words per sentence\n",
    "\n",
    "Sentence length variation\n",
    "\n",
    "Lexical diversity, which is a measure of the richness of the authorâ€™s vocabulary\n",
    "\n",
    "Punctuation features:\n",
    "\n",
    "Average number of commas, semicolons and colons per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vectors\n",
    "def LexicalFeatures(reviews_texts):\n",
    "    \"\"\"\n",
    "    Compute feature vectors for word and punctuation features\n",
    "    \"\"\"\n",
    "    num_reviews = len(reviews_texts)\n",
    "    fvs_lexical = np.zeros((len(reviews_texts), 3), np.float64)\n",
    "    fvs_punct = np.zeros((len(reviews_texts), 3), np.float64)\n",
    "    for e, ch_text in enumerate(reviews_texts):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(ch_text.lower())\n",
    "        words = word_tokenizer.tokenize(ch_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                   for s in sentences])\n",
    " \n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    " \n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "    \n",
    "    return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(all_reviews_texts, reviews_texts):\n",
    "    \"\"\"\n",
    "    Compute the bag of words feature vectors, based on the most common words\n",
    "     in the whole book\n",
    "    \"\"\"\n",
    "    # get most common words in the whole book\n",
    "    NUM_TOP_WORDS = 10\n",
    "    all_tokens = nltk.word_tokenize(all_reviews_texts)\n",
    "    fdist = nltk.FreqDist(all_tokens)\n",
    "    vocab = list(fdist.keys())[:NUM_TOP_WORDS]\n",
    " \n",
    "    # use sklearn to create the bag for words feature vector for each chapter\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(reviews_texts).toarray().astype(np.float64)\n",
    " \n",
    "    # normalise by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "    \n",
    "    # deal with nan\n",
    "    #Obtain mean of columns as you need, nanmean is just convenient.\n",
    "    col_mean = np.nanmean(fvs_bow, axis=0)\n",
    "    #Find indicies that you need to replace\n",
    "    inds = np.where(np.isnan(fvs_bow))\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    fvs_bow[inds] = np.take(col_mean, inds[1])\n",
    "    \n",
    "    return fvs_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures(reviews_texts):\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    # get part of speech for each token in each review\n",
    "    def token_to_pos(ch):\n",
    "        tokens = nltk.word_tokenize(ch)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "    review_pos = [token_to_pos(ch) for ch in reviews_texts]\n",
    " \n",
    "    # count frequencies for common POS types\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list] for ch in review_pos]).astype(np.float64)\n",
    " \n",
    "    # normalise by dividing each row by number of tokens in the review\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in review_pos])]\n",
    "    \n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from scipy.cluster.vq import whiten\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs_lexical_train = np.array([], dtype=np.int64).reshape(0,3)\n",
    "fvs_punct_train = np.array([], dtype=np.int64).reshape(0,3)\n",
    "fvs_bow_train = np.array([], dtype=np.int64).reshape(0,10)\n",
    "fvs_syntax_train = np.array([], dtype=np.int64).reshape(0,6)\n",
    "true_label_train = np.array([], dtype=np.int64).reshape(0,1)\n",
    "for lit in np.arange(len(reviewers_literature)):\n",
    "    # lexical and punct\n",
    "    fvs_lexical_train_cur, fvs_punct_train_cur = LexicalFeatures(reviewers_literature[lit])\n",
    "    fvs_lexical_train = np.concatenate((fvs_lexical_train,fvs_lexical_train_cur))\n",
    "    fvs_punct_train = np.concatenate((fvs_punct_train,fvs_punct_train_cur))\n",
    "    # bag of words\n",
    "    all_literature_texts = ' '.join(reviewers_literature[lit])\n",
    "    fvs_bow_train_cur = BagOfWords(all_literature_texts, reviewers_literature[lit])\n",
    "    fvs_bow_train = np.concatenate((fvs_bow_train,fvs_bow_train_cur))\n",
    "    # syntax\n",
    "    fvs_syntax_train_cur = SyntacticFeatures(reviewers_literature[lit])\n",
    "    fvs_syntax_train = np.concatenate((fvs_syntax_train,fvs_syntax_train_cur))\n",
    "    # label\n",
    "    true_label_train = np.concatenate((true_label_train, lit*np.ones(( len(reviewers_literature[lit]) ,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs_train = np.concatenate((fvs_lexical_train,fvs_punct_train,fvs_bow_train,fvs_syntax_train),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(100)\n",
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, input_size, num_labels):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(LRClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is fvs_dim\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(input_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/200], Loss: 4.4142\n",
      "Epoch: [51/200], Loss: 1.7308\n",
      "Epoch: [101/200], Loss: 0.9651\n",
      "Epoch: [151/200], Loss: 0.6221\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = len(reviewers_literature)\n",
    "INPUT_SIZE = fvs_train.shape[1]\n",
    "input_vector = torch.from_numpy(fvs_train)\n",
    "input_vector = input_vector.float()\n",
    "input_vector = input_vector.to(cuda)\n",
    "labels = torch.from_numpy(true_label_train)\n",
    "labels = labels.view(-1)\n",
    "labels = labels.long()\n",
    "labels = labels.to(cuda)\n",
    "model = LRClassifier(INPUT_SIZE, NUM_LABELS)\n",
    "model = model.to(cuda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):    \n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_vector)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if epoch % 50 == 0:\n",
    "        print ('Epoch: [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs_lexical_train = np.array([], dtype=np.int64).reshape(0,3)\n",
    "fvs_punct_train = np.array([], dtype=np.int64).reshape(0,3)\n",
    "fvs_bow_train = np.array([], dtype=np.int64).reshape(0,10)\n",
    "fvs_syntax_train = np.array([], dtype=np.int64).reshape(0,6)\n",
    "true_label_train = np.array([], dtype=np.int64).reshape(0,1)\n",
    "for lit in np.arange(len(reviewers_literature)):\n",
    "    # lexical and punct\n",
    "    fvs_lexical_train_cur, fvs_punct_train_cur = LexicalFeatures(reviewers_literature[lit])\n",
    "    fvs_lexical_train = np.concatenate((fvs_lexical_train,fvs_lexical_train_cur))\n",
    "    fvs_punct_train = np.concatenate((fvs_punct_train,fvs_punct_train_cur))\n",
    "    # bag of words\n",
    "    all_literature_texts = ' '.join(reviewers_literature[lit])\n",
    "    fvs_bow_train_cur = BagOfWords(all_literature_texts, reviewers_literature[lit])\n",
    "    fvs_bow_train = np.concatenate((fvs_bow_train,fvs_bow_train_cur))\n",
    "    # syntax\n",
    "    fvs_syntax_train_cur = SyntacticFeatures(reviewers_literature[lit])\n",
    "    fvs_syntax_train = np.concatenate((fvs_syntax_train,fvs_syntax_train_cur))\n",
    "    # label\n",
    "    true_label_train = np.concatenate((true_label_train, lit*np.ones(( len(reviewers_literature[lit]) ,1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
