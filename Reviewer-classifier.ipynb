{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"leading_reviewer_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "reviewer_path_list = glob(data_path+\"*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use 10 reviews for each reviewer (pick out the 10 largest size of reviews)\n",
    "reviewers = []\n",
    "reviewers_review = []\n",
    "reviewers_literature = []\n",
    "for reviewer_path in reviewer_path_list:\n",
    "    reviewers.append(re.search(r'leading_reviewer_data\\\\([a-zA-Z_-]*)\\\\',reviewer_path).group(1))\n",
    "    current_reviewer_review = [f for f in os.listdir(reviewer_path) if '.txt' in f]\n",
    "    # select the 10 largest size of reviews\n",
    "    current_selected_review_index = np.argsort([os.stat(reviewer_path+f).st_size for f in os.listdir(reviewer_path) if '.txt' in f])[::-1][:10]\n",
    "    current_reviewer_review_selected = list(np.array(current_reviewer_review)[ind])\n",
    "    # get the review texts\n",
    "    review_texts = []\n",
    "    for nm in current_reviewer_review_selected:\n",
    "        with open(reviewer_path+nm, encoding=\"utf8\") as f:\n",
    "            text_to_append = f.read().replace('\\n',' ')\n",
    "            review_texts.append(text_to_append)\n",
    "    reviewers_review.append(review_texts)\n",
    "    \n",
    "    current_reviewer_literature = [f for f in os.listdir(reviewer_path+\"training_data\\\\\") if '.txt' in f]\n",
    "    # get the literature texts\n",
    "    literature_texts = []\n",
    "    for nm in current_reviewer_literature:\n",
    "        with open(reviewer_path+\"training_data\\\\\"+nm, encoding=\"utf8\") as f:\n",
    "            text_to_append = f.read().replace('\\n',' ')\n",
    "            literature_texts.append(text_to_append)\n",
    "    reviewers_literature.append(literature_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build training set and test set\n",
    "## Randomly picking the test data from the whole dataset\n",
    "total: 25 reviewers\n",
    "\n",
    "each reviewer has 10 reviews and 10 literatures.\n",
    "\n",
    "Test set: all the reviews\n",
    "\n",
    "Training set: all the literatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical and punctuation features\n",
    "\n",
    "Lexical features:\n",
    "\n",
    "The average number of words per sentence\n",
    "\n",
    "Sentence length variation\n",
    "\n",
    "Lexical diversity, which is a measure of the richness of the authorâ€™s vocabulary\n",
    "\n",
    "Punctuation features:\n",
    "\n",
    "Average number of commas, semicolons and colons per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vectors\n",
    "def LexicalFeatures(reviews_texts):\n",
    "    \"\"\"\n",
    "    Compute feature vectors for word and punctuation features\n",
    "    \"\"\"\n",
    "    num_reviews = len(reviews_texts)\n",
    "    fvs_lexical = np.zeros((len(reviews_texts), 3), np.float64)\n",
    "    fvs_punct = np.zeros((len(reviews_texts), 3), np.float64)\n",
    "    for e, ch_text in enumerate(reviews_texts):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(ch_text.lower())\n",
    "        words = word_tokenizer.tokenize(ch_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                   for s in sentences])\n",
    " \n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    " \n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "    \n",
    "    return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(all_reviews_texts, reviews_texts):\n",
    "    \"\"\"\n",
    "    Compute the bag of words feature vectors, based on the most common words\n",
    "     in the whole book\n",
    "    \"\"\"\n",
    "    # get most common words in the whole book\n",
    "    NUM_TOP_WORDS = 10\n",
    "    all_tokens = nltk.word_tokenize(all_reviews_texts)\n",
    "    fdist = nltk.FreqDist(all_tokens)\n",
    "    vocab = list(fdist.keys())[:NUM_TOP_WORDS]\n",
    " \n",
    "    # use sklearn to create the bag for words feature vector for each chapter\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(reviews_texts).toarray().astype(np.float64)\n",
    " \n",
    "    # normalise by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "    \n",
    "    # deal with nan\n",
    "    #Obtain mean of columns as you need, nanmean is just convenient.\n",
    "    col_mean = np.nanmean(fvs_bow, axis=0)\n",
    "    #Find indicies that you need to replace\n",
    "    inds = np.where(np.isnan(fvs_bow))\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    fvs_bow[inds] = np.take(col_mean, inds[1])\n",
    "    \n",
    "    return fvs_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures(reviews_texts):\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    # get part of speech for each token in each review\n",
    "    def token_to_pos(ch):\n",
    "        tokens = nltk.word_tokenize(ch)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "    review_pos = [token_to_pos(ch) for ch in reviews_texts]\n",
    " \n",
    "    # count frequencies for common POS types\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list] for ch in review_pos]).astype(np.float64)\n",
    " \n",
    "    # normalise by dividing each row by number of tokens in the review\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in review_pos])]\n",
    "    \n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from scipy.cluster.vq import whiten\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs_lexical_train = np.array([], dtype=np.int64).reshape(0,3)\n",
    "fvs_punct_train = np.array([], dtype=np.int64).reshape(0,3)\n",
    "fvs_bow_train = np.array([], dtype=np.int64).reshape(0,10)\n",
    "fvs_syntax_train = np.array([], dtype=np.int64).reshape(0,6)\n",
    "true_label_train = np.array([], dtype=np.int64).reshape(0,1)\n",
    "for lit in np.arange(len(reviewers_literature)):\n",
    "    # lexical and punct\n",
    "    fvs_lexical_train_cur, fvs_punct_train_cur = LexicalFeatures(reviewers_literature[lit])\n",
    "    fvs_lexical_train = np.concatenate((fvs_lexical_train,fvs_lexical_train_cur))\n",
    "    fvs_punct_train = np.concatenate((fvs_punct_train,fvs_punct_train_cur))\n",
    "    # bag of words\n",
    "    all_literature_texts = ' '.join(reviewers_literature[lit])\n",
    "    fvs_bow_train_cur = BagOfWords(all_literature_texts, reviewers_literature[lit])\n",
    "    fvs_bow_train = np.concatenate((fvs_bow_train,fvs_bow_train_cur))\n",
    "    # syntax\n",
    "    fvs_syntax_train_cur = SyntacticFeatures(reviewers_literature[lit])\n",
    "    fvs_syntax_train = np.concatenate((fvs_syntax_train,fvs_syntax_train_cur))\n",
    "    # label\n",
    "    true_label_train = np.concatenate((true_label_train, lit*np.ones(( len(reviewers_literature[lit]) ,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs_train = np.concatenate((fvs_lexical_train,fvs_punct_train,fvs_bow_train,fvs_syntax_train),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
